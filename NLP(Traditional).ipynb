{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe8d7bfd-9078-486d-a677-b3bb1d9c9d6e",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37d6d588-6ecb-4437-9697-22c793c7ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a9a39db-4249-44be-8fd8-530d539d58a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2401</th>\n",
       "      <th>Borderlands</th>\n",
       "      <th>Positive</th>\n",
       "      <th>im getting on borderlands and i will murder you all ,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2401  Borderlands  Positive  \\\n",
       "0  2401  Borderlands  Positive   \n",
       "1  2401  Borderlands  Positive   \n",
       "2  2401  Borderlands  Positive   \n",
       "3  2401  Borderlands  Positive   \n",
       "4  2401  Borderlands  Positive   \n",
       "\n",
       "  im getting on borderlands and i will murder you all ,  \n",
       "0  I am coming to the borders and I will kill you...     \n",
       "1  im getting on borderlands and i will kill you ...     \n",
       "2  im coming on borderlands and i will murder you...     \n",
       "3  im getting on borderlands 2 and i will murder ...     \n",
       "4  im getting into borderlands and i can murder y...     "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "data = pd.read_csv('twitter_training.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e289ee9-380e-426b-804a-e36c84fe00b4",
   "metadata": {},
   "source": [
    "### ABOUT DATASET\n",
    "#### The twitter sentiment dataset from kaggle is a data containing posts of people's review over a video game  \"Borderlands\" \n",
    "Each row in the dataset typically includes:\n",
    "\n",
    "ID: A unique identifier for the tweet.\n",
    "\n",
    "Topic: The subject of the tweet (e.g., \"Borderlands\").\n",
    "\n",
    "Sentiment: The sentiment expressed in the tweet (e.g., Positive, Negative, Neutral).\n",
    "\n",
    "Tweet: The text content of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2472fb0f-d77d-41d8-b2ba-dbe5b9303389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74681, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "560e3899-29b6-42c3-a761-7bbc6eb12318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2401</th>\n",
       "      <th>Borderlands</th>\n",
       "      <th>Positive</th>\n",
       "      <th>im getting on borderlands and i will murder you all ,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2405</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Negative</td>\n",
       "      <td>the biggest dissappoinment in my life came out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2405</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Negative</td>\n",
       "      <td>The biggest disappointment of my life came a y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2405</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Negative</td>\n",
       "      <td>The biggest disappointment of my life came a y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2405</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Negative</td>\n",
       "      <td>the biggest dissappoinment in my life coming o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2405</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Negative</td>\n",
       "      <td>For the biggest male dissappoinment in my life...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    2401  Borderlands  Positive  \\\n",
       "23  2405  Borderlands  Negative   \n",
       "24  2405  Borderlands  Negative   \n",
       "25  2405  Borderlands  Negative   \n",
       "26  2405  Borderlands  Negative   \n",
       "27  2405  Borderlands  Negative   \n",
       "\n",
       "   im getting on borderlands and i will murder you all ,  \n",
       "23  the biggest dissappoinment in my life came out...     \n",
       "24  The biggest disappointment of my life came a y...     \n",
       "25  The biggest disappointment of my life came a y...     \n",
       "26  the biggest dissappoinment in my life coming o...     \n",
       "27  For the biggest male dissappoinment in my life...     "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['Positive'] == 'Negative'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14073799-a356-46e3-94b9-11caa9b82bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Borderlands</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  Borderlands Sentiment  \\\n",
       "0    2401  Borderlands  Positive   \n",
       "1    2401  Borderlands  Positive   \n",
       "\n",
       "                                               posts  \n",
       "0  I am coming to the borders and I will kill you...  \n",
       "1  im getting on borderlands and i will kill you ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rename column \n",
    "data.columns = ['UserID', 'Borderlands', 'Sentiment',\n",
    "       'posts']\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdb9a61-5198-4619-8911-fa127026a46c",
   "metadata": {},
   "source": [
    "## Creating a traditional NLP, using the above imported dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfe2d914-9978-4b5f-ae9c-19ce005e9d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserID           0\n",
       "Borderlands      0\n",
       "Sentiment        0\n",
       "posts          686\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for missing values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0e62ee-1bd0-41ff-b304-5df4d914bdaf",
   "metadata": {},
   "source": [
    "### first we preprocess the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ff0ef2-40df-45a0-9dd1-66f12e98ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nlk     #installing package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6cd6ee3-dbf6-4764-831e-c43c6aa2a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize    #importing our tokenization packages from NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ae2c3-fafe-4dd6-ac01-a56497667a07",
   "metadata": {},
   "source": [
    "#### A little demo to show how NLTK punkt tokenizes data before we proceed with our actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c422706a-d292-4d63-a1fa-60ba519f8aa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['baby',\n",
       "  ',',\n",
       "  'how',\n",
       "  'are',\n",
       "  'you',\n",
       "  'doing',\n",
       "  'today',\n",
       "  '?',\n",
       "  'hope',\n",
       "  'you',\n",
       "  'are',\n",
       "  'good'],\n",
       " ['baby', 'how', 'are', 'you', 'doing', 'today', 'hope', 'you', 'are', 'good'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')               # Download tokenizer models\n",
    "text = \"baby, how are you doing today? hope you are good\"\n",
    "text1 = \"baby how are you doing today hope you are good\"\n",
    "word_token_text, sent_token_text = word_tokenize(text), sent_tokenize(text)\n",
    "word_token_text1, sent_token_text1 = word_tokenize(text1), sent_tokenize(text1)\n",
    "word_token_text, word_token_text1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70cef930-0cef-41ab-9a8d-c354f9726198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['baby, how are you doing today?', 'hope you are good'],\n",
       " ['baby how are you doing today hope you are good'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_token_text, sent_token_text1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c808a2f-54df-4664-8775-5a7cb9ee74e4",
   "metadata": {},
   "source": [
    "### Now we proceed to tokenize our actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be167cde-8da6-4ab0-b33e-626ab299cecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['posts'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a4df2b2-a276-409a-98f7-45a211cc209f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        I am coming to the borders and I will kill you...\n",
       "1        im getting on borderlands and i will kill you ...\n",
       "2        im coming on borderlands and i will murder you...\n",
       "3        im getting on borderlands 2 and i will murder ...\n",
       "4        im getting into borderlands and i can murder y...\n",
       "                               ...                        \n",
       "74676    Just realized that the Windows partition of my...\n",
       "74677    Just realized that my Mac window partition is ...\n",
       "74678    Just realized the windows partition of my Mac ...\n",
       "74679    Just realized between the windows partition of...\n",
       "74680    Just like the windows partition of my Mac is l...\n",
       "Name: posts, Length: 74681, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change data type in case the data is not of type string or object\n",
    "data['posts'].astype('str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d132d76-eb9f-4cbb-8499-31fecc7fae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70c3c41c-aecc-4991-a43f-3cf9197db342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Borderlands</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74681 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserID  Borderlands Sentiment  \\\n",
       "0        2401  Borderlands  Positive   \n",
       "1        2401  Borderlands  Positive   \n",
       "2        2401  Borderlands  Positive   \n",
       "3        2401  Borderlands  Positive   \n",
       "4        2401  Borderlands  Positive   \n",
       "...       ...          ...       ...   \n",
       "74676    9200       Nvidia  Positive   \n",
       "74677    9200       Nvidia  Positive   \n",
       "74678    9200       Nvidia  Positive   \n",
       "74679    9200       Nvidia  Positive   \n",
       "74680    9200       Nvidia  Positive   \n",
       "\n",
       "                                                   posts  \n",
       "0      I am coming to the borders and I will kill you...  \n",
       "1      im getting on borderlands and i will kill you ...  \n",
       "2      im coming on borderlands and i will murder you...  \n",
       "3      im getting on borderlands 2 and i will murder ...  \n",
       "4      im getting into borderlands and i can murder y...  \n",
       "...                                                  ...  \n",
       "74676  Just realized that the Windows partition of my...  \n",
       "74677  Just realized that my Mac window partition is ...  \n",
       "74678  Just realized the windows partition of my Mac ...  \n",
       "74679  Just realized between the windows partition of...  \n",
       "74680  Just like the windows partition of my Mac is l...  \n",
       "\n",
       "[74681 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f829dcdd-ff02-4d40-9a09-0832130b278d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "686"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['posts'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65223db5-3bb2-4a1f-b654-6a52092d059b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Borderlands'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdc210ad-21e8-47ef-8781-98e15b5ee1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8eb4c59-79f6-437c-919b-faae9a04cd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10420\\2979981320.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data1['word_token'] = data1['posts'].apply(lambda x: word_tokenize(x))   #applying tokenizer i.e to split each word in text\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_token</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[I, am, coming, to, the, borders, and, I, will...</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[im, getting, on, borderlands, and, i, will, k...</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[im, coming, on, borderlands, and, i, will, mu...</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[im, getting, on, borderlands, 2, and, i, will...</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[im, getting, into, borderlands, and, i, can, ...</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          word_token  \\\n",
       "0  [I, am, coming, to, the, borders, and, I, will...   \n",
       "1  [im, getting, on, borderlands, and, i, will, k...   \n",
       "2  [im, coming, on, borderlands, and, i, will, mu...   \n",
       "3  [im, getting, on, borderlands, 2, and, i, will...   \n",
       "4  [im, getting, into, borderlands, and, i, can, ...   \n",
       "\n",
       "                                               posts  \n",
       "0  I am coming to the borders and I will kill you...  \n",
       "1  im getting on borderlands and i will kill you ...  \n",
       "2  im coming on borderlands and i will murder you...  \n",
       "3  im getting on borderlands 2 and i will murder ...  \n",
       "4  im getting into borderlands and i can murder y...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['word_token'] = data1['posts'].apply(lambda x: word_tokenize(x))   #applying tokenizer i.e to split each word in text\n",
    "data1[['word_token', 'posts']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdd9f315-9698-4ba8-bfc2-9e39c29e0846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Borderlands</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>posts</th>\n",
       "      <th>word_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>[I, am, coming, to, the, borders, and, I, will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>[im, getting, on, borderlands, and, i, will, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>[im, coming, on, borderlands, and, i, will, mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>[im, getting, on, borderlands, 2, and, i, will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "      <td>[im, getting, into, borderlands, and, i, can, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  Borderlands Sentiment  \\\n",
       "0    2401  Borderlands  Positive   \n",
       "1    2401  Borderlands  Positive   \n",
       "2    2401  Borderlands  Positive   \n",
       "3    2401  Borderlands  Positive   \n",
       "4    2401  Borderlands  Positive   \n",
       "\n",
       "                                               posts  \\\n",
       "0  I am coming to the borders and I will kill you...   \n",
       "1  im getting on borderlands and i will kill you ...   \n",
       "2  im coming on borderlands and i will murder you...   \n",
       "3  im getting on borderlands 2 and i will murder ...   \n",
       "4  im getting into borderlands and i can murder y...   \n",
       "\n",
       "                                          word_token  \n",
       "0  [I, am, coming, to, the, borders, and, I, will...  \n",
       "1  [im, getting, on, borderlands, and, i, will, k...  \n",
       "2  [im, coming, on, borderlands, and, i, will, mu...  \n",
       "3  [im, getting, on, borderlands, 2, and, i, will...  \n",
       "4  [im, getting, into, borderlands, and, i, can, ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d63e07da-65f1-4be6-b1b5-057232de0630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 73995 entries, 0 to 74680\n",
      "Series name: posts\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "73995 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    " data1['posts'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132e7d80-2ca2-4f2f-9fe7-252f6d744405",
   "metadata": {},
   "source": [
    "### After tokenizing text, we proceed to remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fac498db-206e-4c2e-b3fa-a5e95a51a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library for stop word removal\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')    #download stopwords dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ec982b7-6be5-4fe1-8c19-ada34a184731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10420\\3707290888.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data1['filtered_tokens'] = data1['word_token'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>filtered_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>[I, coming, borders, I, kill, ,]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>[im, getting, borderlands, kill, ,]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>[im, coming, borderlands, murder, ,]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>[im, getting, borderlands, 2, murder, ,]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "      <td>[im, getting, borderlands, murder, ,]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               posts  \\\n",
       "0  I am coming to the borders and I will kill you...   \n",
       "1  im getting on borderlands and i will kill you ...   \n",
       "2  im coming on borderlands and i will murder you...   \n",
       "3  im getting on borderlands 2 and i will murder ...   \n",
       "4  im getting into borderlands and i can murder y...   \n",
       "\n",
       "                            filtered_tokens  \n",
       "0          [I, coming, borders, I, kill, ,]  \n",
       "1       [im, getting, borderlands, kill, ,]  \n",
       "2      [im, coming, borderlands, murder, ,]  \n",
       "3  [im, getting, borderlands, 2, murder, ,]  \n",
       "4     [im, getting, borderlands, murder, ,]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))    #load english stopwords\n",
    "data1['filtered_tokens'] = data1['word_token'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "data1[['posts', 'filtered_tokens']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f652664-3add-4f5c-a0df-b7f67cad9162",
   "metadata": {},
   "source": [
    "### Haven removed stop words, we proceed to stemmatize the filtered(stop_words removed) tokens and return the data to its text form rather than the list it currently exist as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9425ea5-7fc3-4e21-ac12-65f11dac39a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10420\\3195757104.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data1['stemmed_text'] = data1['posts'].apply(stem_text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>stemmed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>i am come to the border and i will kill you all ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>im get on borderland and i will kill you all ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>im come on borderland and i will murder you all ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>im get on borderland 2 and i will murder you m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "      <td>im get into borderland and i can murder you all ,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               posts  \\\n",
       "0  I am coming to the borders and I will kill you...   \n",
       "1  im getting on borderlands and i will kill you ...   \n",
       "2  im coming on borderlands and i will murder you...   \n",
       "3  im getting on borderlands 2 and i will murder ...   \n",
       "4  im getting into borderlands and i can murder y...   \n",
       "\n",
       "                                        stemmed_text  \n",
       "0  i am come to the border and i will kill you all ,  \n",
       "1     im get on borderland and i will kill you all ,  \n",
       "2  im come on borderland and i will murder you all ,  \n",
       "3  im get on borderland 2 and i will murder you m...  \n",
       "4  im get into borderland and i can murder you all ,  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import stemmatization library\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "# Function to apply stemming and convert back to text\n",
    "def stem_text(text):\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]  # Apply stemming\n",
    "    return \" \".join(stemmed_tokens)  # Convert back to text\n",
    "\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "data1['stemmed_text'] = data1['posts'].apply(stem_text)\n",
    "\n",
    "data1[['posts', 'stemmed_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9258115a-6f48-4bfd-9864-894456773442",
   "metadata": {},
   "source": [
    "### Next to do is tag each word in text based on their individual part of speech (POS TAGGING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2585e6e5-026c-4079-9ffb-b092e355c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library for POS tagging\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7836159a-f3bd-4f5e-a836-8c7a877cb71d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data1['POS_TAG'] = data1['stemmed_token'].apply(lambda a: nltk.pos_tag(a))     #apply POS tagging to each word \n",
    "# data1[['POS_TAG', 'stemmed_token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d60acdf4-1e44-4604-9b1d-48fdc3788de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[['POS_TAG', 'posts']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f57794-c36d-477d-8174-8b95afe8dbff",
   "metadata": {},
   "source": [
    "### Now we have to convert the stemmed tokenized data back to text, else the data remains as a list of data with each text as a separate entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a79d6-3a95-4ee5-a67d-42a76dfbd60d",
   "metadata": {},
   "source": [
    "### VECTORIZE TEXT DATA \n",
    "#### we'd use TF-IDF since we are trying to analysis sentiment\n",
    "TF-IDF weighs words based on importance, reducing the impact of frequent words like \"the\", \"is\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04dd8820-1d1d-4897-849e-07d17f28b4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['00' '000' '00016' ... 'zuckerberg' 'безопасно' 'яй']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<73995x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1121295 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import text vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  #max_features=5000 → Keep only the most important 5,000 words.\n",
    "x = data1['stemmed_text']\n",
    "feature_matrix = vectorizer.fit_transform(x)               # Sparse format (efficient!)\n",
    "\n",
    "# Print TF-IDF matrix\n",
    "print(\"Feature names:\", vectorizer.get_feature_names_out())\n",
    "# print(feature_matrix.toarray())  \n",
    "'''\n",
    "👆this \"feature_matrix.toarray()\" converts the sparse matrix to a dense array of 74681 × 25441 = 1.9 billion values, even if most are zeros.\n",
    "Thereby overwhelming the system and its memory.\n",
    "\n",
    "to handle this we avoid using \".toarray()\", we Keep It as a Sparse Matrix bt leaving it like that. i.e after \"vectorizer.fit_transform(data)\"\n",
    "\n",
    "However, If you want to view the data in a DataFrame but avoid .toarray(), use pd.DataFrame.sparse.from_spmatrix():\n",
    "'''\n",
    "\n",
    "matrix = feature_matrix  # Check dimensions\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14f20ee0-48df-400f-8430-fda75283f1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73995, 5000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da562137-46d0-4c54-9705-236d7a040cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "feature_matrix[:1][0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff4f4b-308c-463a-a32b-ab36340a4216",
   "metadata": {},
   "source": [
    "### Now we proceed to build our model but first we have to assign our x and y values and split into train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b68c44a8-fd54-44fe-b329-a77279f087eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning x and y\n",
    "x1, y = matrix, data1['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea2f1234-2ce7-4b13-8181-b125143e2f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<73995x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1121295 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b929cbd8-904f-4f52-8f4e-31bd14c46c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 10 stored elements in Compressed Sparse Row format>,\n",
       " 0    Positive\n",
       " Name: Sentiment, dtype: object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[:1],y[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40cb0409-4087-46cc-ad17-63486ce5bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into trian test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x1,y, random_state = 69, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9174dcf6-7f23-4b97-aead-3ff97723cf1a",
   "metadata": {},
   "source": [
    "## Import model (naive bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4a43923-ca27-4970-914e-73da6e8051bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e4e600e-8795-423f-b590-e13cbe62531f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51796, 5000), (51796,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9c02a49-8f1d-4ba5-b955-7318c9fc5032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<51796x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 783370 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0efa1967-3ca0-4e71-9542-4b722af5e146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBmodel = MultinomialNB()\n",
    "NBmodel.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "864cc0e8-f6f3-41d0-85c6-23ef02d58149",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid, x_test, y_valid, y_test = train_test_split(x_test, y_test, train_size = 0.5, random_state =49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f62f453c-dc33-49e4-9cbc-694a812d99d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6459140463104784"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBmodel.score(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c73b8fc-d15e-43bb-b5e0-92a9dc85d522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6710170669549772"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBmodel.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a785c4-9115-4d36-afdc-94e3eb64b671",
   "metadata": {},
   "source": [
    "#### Using a simpler model like the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e010f61-caec-4553-a172-7ede55b94278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(multi_class='multinomial')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try using logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LRmodel= LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=100)\n",
    "LRmodel.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb163330-3cac-4e6e-b7c2-b25591e72ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7400185342497491, 0.6896116767276331)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRmodel.score(x_train, y_train), LRmodel.score(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ccdefb-1225-481e-bba3-6e992a04f959",
   "metadata": {},
   "source": [
    "#### clearly our model has failed to capture underlying patterns thereby leading to low score for both train and test,\n",
    "so we trace back to our feature engineering to reprocess our data.\n",
    " The issue might be with how text is converted into numerical features. Try the following:\n",
    "\n",
    "✅ Increase n-grams: Instead of just single words (unigrams), use bigrams or trigrams in your TF-IDF vectorizer.\n",
    "\n",
    "✅ Try Word Embeddings: Instead of just TF-IDF, try Word2Vec, GloVe, or FastText to capture semantic meaning.\n",
    "\n",
    "✅ Use Stopword Removal: Removing stopwords helps reduce noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fcbd6ec0-9eee-4f2e-b749-1324df5c45d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['00' '000' '00011' ... 'การออกอากาศของฉ' 'นจาก' 'ℐℓ٥']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<73995x25440 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1207950 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first we change the n-gram TF-IDF parameteer to help the model capture contextual meaning as unigram is the default value and only captures word importance(not context).\n",
    "vectorizer1 = TfidfVectorizer()  \n",
    "\n",
    "feature_matrix1 = vectorizer1.fit_transform(x)               # Sparse format (efficient!)\n",
    "\n",
    "# Print TF-IDF matrix\n",
    "print(\"Feature names:\", vectorizer1.get_feature_names_out())\n",
    "\n",
    "matrix1 = feature_matrix1  # Check dimensions\n",
    "matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0eb54c7e-ffbc-47ce-874e-5000e8f7bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = feature_matrix1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd757c13-d6ed-4968-ad5e-83141fd19a65",
   "metadata": {},
   "source": [
    "### split the newly generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "873dd069-078f-4217-8655-5d87218eb7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train, test and validation\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x2,y, train_size  = 0.7, random_state =65)\n",
    "x_test1, x_valid1, y_test1, y_valid1 = train_test_split(x_test1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "04cfeda7-5e80-4b11-a488-748e49b1bec7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserID         18318\n",
       "Borderlands    18318\n",
       "Sentiment      18318\n",
       "posts          18108\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for data imbalance\n",
    "pd.options.display.max_rows = 53850          #neutral = 18318, negative = 22542, irrelevant = 12990, positive = 20831\n",
    "data[data['Sentiment']== 'Neutral'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "265a9ad9-bf96-4f40-80c2-3eb5fb34dd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(multi_class='multinomial')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRmodel1 = LogisticRegression(multi_class='multinomial', class_weight = 'balanced', solver = 'lbfgs', penalty = 'l2', max_iter=100)\n",
    "#the class_weight as balance is used to balance the classes in the data, thereby handling the imbalance that exist\n",
    "#penalty is used to handle the high dimensionality of the data. Instead of going through the long part of using PCA to reduce data dimensionality\n",
    "LRmodel.fit(x_train1, y_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d53c275-ee07-4bd6-bc40-e7846727cbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8172638813808016, 0.7484684684684685)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRmodel.score(x_train1, y_train1),LRmodel.score(x_valid1, y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e078b6e-8085-4899-9ac1-6ce189725a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dde2e00-71d1-4ff0-b5c2-dbf82ec8e13a",
   "metadata": {},
   "source": [
    "### Instead of TF-IDF,we will use Word2Vec, GloVe or FastText embeddings with traditional models like SVM, LR as the TF-IDF was unable to observe underlying pattern in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61cd1a64-163e-479d-9e66-a1dc8f6d403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim    #import model for word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca11b7ba-58e9-4d9b-ac41-602896bd8081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9953fb7-10d0-4ba8-9112-d988132a24e3",
   "metadata": {},
   "source": [
    "##### Note: word2Vec takes tokenized text and not stemmed text, so we parse the tokenized text to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "850c67a5-1f67-4cf8-9e20-0db8bdd12adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEXPLAINING CODE:\\n\"vector_size=100\" Defines the size (dimensions) of word embeddings. It sets the number of dimensions for each word vector.\\nHigher values capture more semantic information but require more training data.\\nLower values train faster but may miss details.\\n\\n\"window=5\" Defines the context window size. It controls how many words before and after the target word are considered as context.\\nA larger window (e.g., window=10) captures more context but may introduce noise.\\nA smaller window (e.g., window=2) focuses on local dependencies (like phrases).\\n\\n\\n\\n\"min_count=1\" Defines the minimum number of occurrences a word must have to be included in training\\nmin_count=1 → Include all words (even rare ones).\\nmin_count=5 → Ignore words that appear less than 5 times\\nwhy use \"min_count\" Higher values (e.g., 5, 10): Remove rare words to speed up training and improve generalization.\\nLower values (e.g., 1, 2): Keep all words, which may be useful for small datasets.\\n\\n\"workers=4\" Defines the number of CPU threads used for training\\nMore workers = Faster training (if your CPU has multiple cores).\\nRecommended value = Number of CPU cores available.\\ne.g If you have a quad-core CPU, setting workers=4 uses all 4 cores for training.\\nIf you set workers=1, it will train much slower.\\nCommon values for \"workers\"  :  workers=1 (single-core, slow)\\n                                workers=4 (quad-core, recommended)\\n                                workers=8+ (high-performance machines)\\n\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences = data1['filtered_tokens'], vector_size = 100, window=3, min_count=1, workers=4)\n",
    "'''\n",
    "EXPLAINING CODE:\n",
    "\"vector_size=100\" Defines the size (dimensions) of word embeddings. It sets the number of dimensions for each word vector.\n",
    "Higher values capture more semantic information but require more training data.\n",
    "Lower values train faster but may miss details.\n",
    "\n",
    "\"window=5\" Defines the context window size. It controls how many words before and after the target word are considered as context.\n",
    "A larger window (e.g., window=10) captures more context but may introduce noise.\n",
    "A smaller window (e.g., window=2) focuses on local dependencies (like phrases).\n",
    "\n",
    "\n",
    "\n",
    "\"min_count=1\" Defines the minimum number of occurrences a word must have to be included in training\n",
    "min_count=1 → Include all words (even rare ones).\n",
    "min_count=5 → Ignore words that appear less than 5 times\n",
    "why use \"min_count\" Higher values (e.g., 5, 10): Remove rare words to speed up training and improve generalization.\n",
    "Lower values (e.g., 1, 2): Keep all words, which may be useful for small datasets.\n",
    "\n",
    "\"workers=4\" Defines the number of CPU threads used for training\n",
    "More workers = Faster training (if your CPU has multiple cores).\n",
    "Recommended value = Number of CPU cores available.\n",
    "e.g If you have a quad-core CPU, setting workers=4 uses all 4 cores for training.\n",
    "If you set workers=1, it will train much slower.\n",
    "Common values for \"workers\"  :  workers=1 (single-core, slow)\n",
    "                                workers=4 (quad-core, recommended)\n",
    "                                workers=8+ (high-performance machines)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cfad703f-d80a-4708-b3a5-780046ec78c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10420\\2855378190.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data1[\"vector\"] = data1[\"filtered_tokens\"].apply(lambda xx: sentence_to_vector(xx, word2vec_model))\n"
     ]
    }
   ],
   "source": [
    "# Function to convert a sentence to a vector\n",
    "def sentence_to_vector(tokens, model):\n",
    "    word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(100)  # Return zero vector if no words in vocab\n",
    "    return np.mean(word_vectors, axis=0)  # Average of word vectors\n",
    "\n",
    "# Convert all sentences to vectors\n",
    "data1[\"vector\"] = data1[\"filtered_tokens\"].apply(lambda xx: sentence_to_vector(xx, word2vec_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9e9b2ada-cc87-4400-9050-1e9a6dde05dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Borderlands</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>posts</th>\n",
       "      <th>word_token</th>\n",
       "      <th>filtered_tokens</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>[I, am, coming, to, the, borders, and, I, will...</td>\n",
       "      <td>[I, coming, borders, I, kill, ,]</td>\n",
       "      <td>i am come to the border and i will kill you all ,</td>\n",
       "      <td>[0.60630983, 1.0543424, -0.8493102, -0.7768115...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>[im, getting, on, borderlands, and, i, will, k...</td>\n",
       "      <td>[im, getting, borderlands, kill, ,]</td>\n",
       "      <td>im get on borderland and i will kill you all ,</td>\n",
       "      <td>[1.5475639, 0.74700814, -0.86170673, -0.891654...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  Borderlands Sentiment  \\\n",
       "0    2401  Borderlands  Positive   \n",
       "1    2401  Borderlands  Positive   \n",
       "\n",
       "                                               posts  \\\n",
       "0  I am coming to the borders and I will kill you...   \n",
       "1  im getting on borderlands and i will kill you ...   \n",
       "\n",
       "                                          word_token  \\\n",
       "0  [I, am, coming, to, the, borders, and, I, will...   \n",
       "1  [im, getting, on, borderlands, and, i, will, k...   \n",
       "\n",
       "                       filtered_tokens  \\\n",
       "0     [I, coming, borders, I, kill, ,]   \n",
       "1  [im, getting, borderlands, kill, ,]   \n",
       "\n",
       "                                        stemmed_text  \\\n",
       "0  i am come to the border and i will kill you all ,   \n",
       "1     im get on borderland and i will kill you all ,   \n",
       "\n",
       "                                              vector  \n",
       "0  [0.60630983, 1.0543424, -0.8493102, -0.7768115...  \n",
       "1  [1.5475639, 0.74700814, -0.86170673, -0.891654...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7cdfa129-8146-4f21-b1cc-41ab67b7d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "xvec = np.vstack(data1[\"vector\"])  #Feature matrix\n",
    "\n",
    "\n",
    "# Split into Train & Test\n",
    "x_train3, x_test3, y_train3, y_test3 = train_test_split(xvec, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "abbe04bc-c845-4c95-a084-30b255b032cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, multi_class=&#x27;multinomial&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, multi_class=&#x27;multinomial&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(class_weight='balanced', multi_class='multinomial')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#haven proceed our data we fit model \n",
    "LRmodel1 = LogisticRegression(multi_class='multinomial', class_weight = 'balanced', solver = 'lbfgs', penalty = 'l2', max_iter=100)\n",
    "LRmodel1.fit(x_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c42464a7-c7b1-43af-b978-9332699dc3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4928372847324118, 0.4903824496598946)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRmodel1.score(x_train3, y_train3),LRmodel1.score(x_test3, y_test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6210c89-2e85-4644-a0e0-41382f2a4034",
   "metadata": {},
   "source": [
    "## Our traditional NLP functioned better when we used the nltk text features than when we usedd the word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92721544-ed08-4294-a226-dd3bb163830e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62630cec-4729-47e6-bfba-3eb983b30a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99f0632b-2211-4b27-8033-854284657d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize data\n",
    "import nltk   #download tokenize package\n",
    "# nltk.download('punkt')\n",
    "# data[['word_token']] = data['im getting on borderlands and i will murder you all ,'].apply(word_tokenize)\n",
    "# data[['sent_token']] = data['im getting on borderlands and i will murder you all ,'].apply(sent_tokenize)\n",
    "# print('word-tokenize',word_token )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a97720f0-d1ac-4f7d-a306-8e56e5552a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86b33f6b-8a37-4abd-b92a-2c6e2e81128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "39d52318-f20c-4c4b-ae79-14b0792e138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# text = \"Hello world! Tokenizing text data is fun.\"\n",
    "# tokens = [token.text for token in nlp(text)]\n",
    "\n",
    "\n",
    "# print(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
